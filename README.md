# Software-Development-for-Information-Systems
Project for class of Software Development for Information Systems, DIT UOA, 2024.
Team members:
  - Dimitris Dimochronis | 1115202100034
  - Nikos Pentasilis | 1115202100152 
  - Giorgos Ragkos | 1115202100167

## Build and Run project

### Για την **εκτέλεση** του project:
  ```
  make
  ./bin/vamana config.txt
  ```
### Για την εκτέλεση των **tests**:
  ```
  make test
  ```
### Για τον **καθαρισμό** του directory:
  ```
  make clean
  ```

### Configuration file
Η παροχή παραμέτρων γίνεται μέσω ενός configuration file, το όνομα του οποίου δίνεται ως όρισμα στην εκτέλεση του προγράμματος.<br/>
Το αρχείο πρέπει να έχει την εξής μορφή:
```
dataset=data/ANN_SIFT10K/siftsmall_base.fvecs
query=data/ANN_SIFT10K/siftsmall_query.fvecs
groundtruth=data/ANN_SIFT10K/siftsmall_groundtruth.ivecs
k=100
R=14
L=100
a=1.2
n=
d=
q_idx=
```
- Τα dataset/query/groundtruth προσδιορίζουν το path των συγκεκριμένων αρχείων.
- Τα k, R, L, a είναι οι γνωστοί παράμετροι για τις συναρτήσεις.
- Σε περίπτωση που θέλετε να παράξετε και να τεστάρετε την εφαρμογή σε ένα τυχαίο dataset και query, τότε συμπληρώνετε τα n,d με το μέγεθος του dataset και τη διάσταση των δεδομένων αντίστοιχα.
- Όταν δίνονται αρχεία ως input, μέσω του q_idx μπορείτε:
  - να συμπληρώσετε το index του query στο αρχείο που θέλετε να τεστάρετε (μόνο όταν παρέχετε query file),
  - να το αφήσετε κενό και να τρέξει για όλα τα queries (μόνο όταν παρέχετε query file) ή
  - να γράψετε -2 και να τεστάρετε μόνο ένα query που θα επιλέξει τυχαία το πρόγραμμα (και για query file και για random dataset).

## Workflow
Υπάρχει workflow script μέσω Github Actions ώστε κάθε commit να ελέγχεται αυτόματα για την ορθότητα του κώδικα μέσω των unit tests.

## Parametres and Data types
Στο αρχείο header.h υπάρχουν 2 typedef, ένα για το data type των δεδομένων και ένα για το data type στο οποίο μετράμε την ευκλείδια απόσταση.
Στην παρούσα φάση και μετά από δοκιμές, θεωρήσαμε ότι για το dataset ANN_SIFT10K αρκεί το float. Παρόλα αυτά, αλλάζοντας αυτά τα typedef μπορούμε να τεστάρουμε και άλλους τύπους δεδομένων.

Έπειτα από δοκιμές που κάναμε, για R>log(n), το Recall@k για το ANN_SIFT10K dataset κυμαίνεται μεταξύ 92% και 100%.

Για τις παρακάτω παραμέτρους, οι οποίες υπάρχουν και στον κώδικα, έχουμε ~100% Recall σε 3.6', στα μηχανήματα της σχολής. Στους προσωπικούς μας υπολογιστές, ο χρόνος φτάνει και το 1 λεπτό.
```
@linux15:~/Software-Development-for-Information-Systems$ ./bin/vamana config.txt
 || Dataset: data/ANN_SIFT10K/siftsmall_base.fvecs
 || Query: data/ANN_SIFT10K/siftsmall_query.fvecs
 || Groundtruth: data/ANN_SIFT10K/siftsmall_groundtruth.ivecs
 || k: 100
 || R: 14
 || L: 100
 || a: 1.2
 || Size: 10000
 || Dimension: 128
====================================================================
 || Time taken: 220.484 seconds (= 3.67473 minutes)
 || Size of neighbors found matches k.
 || Number of neighbors found in expected neighbors: 100.
 || Recall@k: 100%.
```

## Graph
- Για την αναπαράσταση του γράφου χρησιμοποιούνται vectors, για απευθείας random indexing.<br/>
- Για κάθε κόμβο, κρατάμε σε ένα struct το vector με τα δεδομένα και ένα unordered set με τους out neighbours. Η επιλογή του set 
  έγινε ώστε να αποφύγουμε τα duplicates χωρίς να απαιτείται χειροκίνητος έλεγχος σε κάθε εισαγωγή.<br/>
- Παρέχονται οι κατάλληλες συναρτήσεις δημιουργίας κόμβου, προσθήκης αυτού σε γράφο καθώς και η εύρεση κόμβου στον γράφο με βάση το data 
  point του.<br/>
- Υπάρχει επίσης συνάρτηση εκτύπωσης των σημείων του γράφου καθώς και εκτύπωσης των out neighbours κόμβου.<br/>

## Helper Functions
- Χρησιμοποιούμε **squared euclidean distance** διότι αυτή είναι και η μετρική του dataset.
- Η **random_permutation** επιστρέφει μια τυχαία μετάθεση των αριθμών από 0 εώς n - 1.
- Η **get_data** επιστρέφει το dataset των κόμβων του δοθέντος γράφου.
- Η **random_dataset** και **random_query** δημιουργούν ένα τυχαίο dataset και query αντίστοιχα.
- Η **check_results** ελέγχει τα αποτελέσματα του greedy search. Εάν πάρει ως όρισμα το **groundtruth** κάνει το απαραίτητο cross-check, αλλιώς υπολογίζει manually τους κοντινότερους γείτονες και τους συγκρίνει με τα αποτελέσματα του αλγορίθμου.

## Vamana indexing
- Για την αρχικοποίηση του τυχαίου γράφου, επιλέγονται για κάθε κόμβο τυχαίοι R διαφορετικοί κόμβοι ώστε να προστεθούν οι κατάλληλες ακμές.
- Ο γράφος διασχίζεται σύμφωνα με το random permutation σ(i).
- Κρατάμε το vector N_out_j_p που αποτελεί το $N_{out}(\sigma (i)) \bigcup \{\sigma (i)\}$ και το ανανεώνουμε κατάλληλα σε κάθε επανάληψη.
- Με την **get_data(V)** παίρνουμε το dataset που υπάρχει στον γράφο V και το στέλνουμε ως όρισμα στη Robust Pruning.

## main
Διαβάζει το **configuration file** και πράττει αναλόγως: αν δίνονται **n, d $\neq$ {}**, τότε χρησιμοποιούνται τα αρχεία που δίνονται από τον χρήστη. Διαφορετικά, παράγεται ένα **τυχαίο dataset** μεγέθους n, με δεδομένα διάστασης d και παράγεται και το αντίστοιχο query point. Δημιουργείται ο γράφος από το **vamana indexing** και διατρέχεται με τη **greedy search**. Στο τέλος, καλεί την **check_results** για να ελέγχξει την ορθότητα των αποτελεσμάτων. Επίσης επεξεργάζεται κατάλληλα το q_idx.
## Greedy Search
- Η **L_m_V** υπολογίζει τα στοιχεία που υπάρχουν στον γράφο L και δεν υπάρχουν στον V.
- Η **greedy_search** επιστρέφει ενα pair που αποτελείται απο δύο γράφους, ο πρώτος γραφος περιέχει τους k κοντινότερους κόμβους απο το σημείο q και ο δεύτερος γράφος περιέχει τα visited nodes.
## Robust Pruning
- Το όρισμα p_node είναι ο κόμβος του σημείου p στον γράφο.
- Προσθέτει στο V όλες τις εξωτερικές ακμές του σημείου p,ελέγχωντας για διπλότυπα.
- Αφαιρεί όλες τις ακμές του p από τον γράφο και έπειτα όσο το V δεν είναι άδειο και το |N_out(p)| δεν είναι ίσο με R,υπολογίζει την απόσταση του p από κάθε σημείο του V και εισάγει το κοντινότερο στο N_out(p).


## Data_forming
- H **bvecs_read** διαβάζει τα δεδομένα των αρχείων με .bvecs format,τα μετατρέπει σε data_t τύπο,τα εισάγει σε dataset το οποίο επιστρέφει όταν διαβαστεί όλο το αρχείο.
- H **fvecs_read** διαβάζει τα δεδομένα των αρχείων με .fvecs format,τα μετατρέπει σε data_t τύπο,τα εισάγει σε dataset το οποίο επιστρέφει όταν διαβαστεί όλο το αρχείο.
- H **ivecs_read** διαβάζει τα δεδομένα των αρχείων με .ivecs format,τα μετατρέπει σε data_t τύπο,τα εισάγει σε dataset το οποίο επιστρέφει όταν διαβαστεί όλο το αρχείο.


## Optimization
- Αποφεύχθηκε όσο γίνεται, η χρήση της **find_data_in_graph** για εύρεση δεδομένων στον γράφο καθώς απαιτεί διάσχιση όλου του γράφου.
- Χρησιμοποιήθηκαν αρκετά **.reserve()** για γρηγορότερο indexing.
- Χρησιμοποιήθηκε το **flag -O3** στο compile.
- Η **medoid** με την χρήση ενός vector για την συνολική απόσταση κάθε σημείου από όλα τα άλλα,υπολογίζει μία φορά την απόσταση κάθε ζευγαριού σημείων,αντί για 2 και την προσθέτει στην συνολιή απόσταση και των 2 σημείων του ζευγαριού.Ουσιαστικά υπολογίζει τον άνω τριγωνικό πίνακα των αποστάσεων όλων των σημείων μεταξύ τους και όχι τον ολόκληρο πίνακα. 


## Unit Testing
Για το unit testing χρησιμοποιήθηκε το **acutest** framework.
- H **test_create_graph_node** τεστάρει την δημιουργία ενος κόμβου.
- Η **test_add_node_to_graph** τεστάρει την εισαγωγή ενος κόμβου σε ένα γράφο.
- Η **test_add_edge_to_graph** τεστάρει την εισαγωγή μίας ακμής στον γράφο.
- Η **test_find_data_in_graph** τεστάρει την ορθότητα της εύρεσης ενος κόμβου με συγκεκριμένα δεδομένα.
- Η **test_euclidean_distance** τεστάρει την ορθότητα της ευκλείδειας απόστασης.
- Η **test_random_permutation** ελέγχει αν έγιναν οι μεταθέσεις.
- Η **test_get_data** ελέγχει αν η ανάκτηση των δεδομένων απο ένα γράφο γίνεται σωστά.
- Η **test_medoid** ελέγχει αν η medoid επιστρέφει την αναμενόμενη τιμή.
- Η **test_greedy_search** ελέγχει αν οι γείτονες που επιστρέφει είναι οι αναμενομένοι.

# Διαμοιρασμός εργασιών μεταξύ των μελών της ομάδας
Ο κύριος διαχωρισμός έγινε ως εξής:
- Δημήτρης Δημοχρόνης: σχεδίαση γράφου και επιλογή δομών, Vamana Indexing, μερικώς optimization και λειτουργικότητα της main,
- Νίκος Πεντασίλης: Pruning, Data Reading και optimization medoid,
- Γιώργος Ράγκος: Greedy Search, Unit Testing, Data Reading.
Ο κώδικας πέρασε από όλους και όλοι κάναμε τροποποιήσεις σε κώδικα άλλου μέλους.<br/>
Έγιναν αρκετά pull requests από την ομάδα καθώς και πολλά sessions για την επίλυση προβλημάτων και την βελτίωση του κώδικα.<br/>
